name: airflow

volumes:
  postgres-db-volume:
  airflow-logs:
  mlflow-runs:
  airflow-artifacts:
  airflow-tmp:

networks:
  airflow:
    driver: bridge

services:
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-airflow} -d ${POSTGRES_DB:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks: [airflow]

  airflow-init:
    build:
      context: ..
      dockerfile: Dockerfile.airflow
    image: loan-default-airflow:latest
    container_name: airflow-init
    depends_on:
      - postgres
    env_file:
      - ../.env
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${POSTGRES_DB:-airflow}
    command: >
      bash -c "airflow db migrate && bash /opt/airflow/create_airflow_user.sh"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../src:/opt/airflow/src
      - ../data:/opt/airflow/data
      - ../keys/gcs-service-account.json:/opt/airflow/keys/gcs-service-account.json  # mount key file only
      - ./artifacts:/opt/airflow/artifacts
      - ../airflow/create_airflow_user.sh:/opt/airflow/create_airflow_user.sh
      - airflow-tmp:/opt/airflow/tmp
    networks: [airflow]
    restart: "no"

  webserver:
    image: loan-default-airflow:latest
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    env_file:
      - ../.env
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${POSTGRES_DB:-airflow}
      PYTHONPATH: /opt/airflow:/opt/airflow/src
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/keys/gcs-service-account.json
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../src:/opt/airflow/src
      - ../data:/opt/airflow/data
      - ../keys/gcs-service-account.json:/opt/airflow/keys/gcs-service-account.json
      - ../requirements.serve.txt:/opt/airflow/requirements.serve.txt
      - ./artifacts:/opt/airflow/artifacts
      - ../airflow/variables.json:/opt/airflow/variables.json   
      - airflow-tmp:/opt/airflow/tmp
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 12
    command: ["airflow", "webserver"]
    networks: [airflow]
    restart: unless-stopped

  scheduler:
    image: loan-default-airflow:latest
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    env_file:
      - ../.env
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${POSTGRES_DB:-airflow}
      PYTHONPATH: /opt/airflow:/opt/airflow/src
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/keys/gcs-service-account.json
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../src:/opt/airflow/src
      - ../data:/opt/airflow/data
      - ../keys/gcs-service-account.json:/opt/airflow/keys/gcs-service-account.json
      - ../requirements.serve.txt:/opt/airflow/requirements.serve.txt
      - ./artifacts:/opt/airflow/artifacts
      - ../airflow/variables.json:/opt/airflow/variables.json  
      - airflow-tmp:/opt/airflow/tmp
    command: ["airflow", "scheduler"]
    networks: [airflow]
    restart: unless-stopped

  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.1.4
    container_name: mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root /mlruns
    volumes:
      - mlflow-runs:/mlruns
    ports:
      - "5000:5000"
    networks: [airflow]
    restart: unless-stopped

  serve:
    build:
      context: ..
      dockerfile: Dockerfile.serve
    image: loan-default-serve:latest
    container_name: airflow-serve
    env_file:
      - ../.env
    environment:
      MODEL_NAME: ${MODEL_NAME:-loan_default_model}
      MODEL_ALIAS: ${MODEL_ALIAS:-staging}
      PORT: 5001
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-http://mlflow:5000}
    volumes:
      - ../entrypoint_serve.sh:/opt/airflow/entrypoint_serve.sh
    command: ["bash", "/opt/airflow/entrypoint_serve.sh"]
    ports:
      - "5001:5001"
    networks: [airflow]
    restart: unless-stopped
