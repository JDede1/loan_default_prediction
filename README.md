# ğŸ¦ Loan Default Prediction â€“ End-to-End MLOps Project

This project implements an **end-to-end MLOps pipeline** for predicting loan defaults using the [LendingClub dataset](https://www.kaggle.com/wordsforthewise/lending-club). The goal is to help financial institutions and lenders **assess borrower risk** and make more informed lending decisions.

---

## ğŸ“Œ Project Overview

This project part of the **[DataTalksClub MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)** capstone. The solution is built with **production readiness** in mind, covering the full machine learning lifecycle:

* **Data ingestion & preprocessing** â€“ clean, transform, and validate input loan application data.
* **Model training & experiment tracking** â€“ train ML models (XGBoost, Logistic Regression, Random Forest), log metrics, parameters, and artifacts using **MLflow**.
* **Model registry & versioning** â€“ manage models in **MLflow Registry**, with separate Staging and Production stages.
* **Model serving** â€“ deploy models via **Dockerized MLflow REST API** for real-time predictions.
* **Batch inference pipeline** â€“ schedule recurring predictions with **Apache Airflow**.
* **Monitoring & drift detection** â€“ track data drift and target drift with **Evidently AI**, generating automated reports.
* **Infrastructure-as-Code (IaC)** â€“ provision cloud resources using **Terraform** on GCP (Google Cloud Platform).
* **CI/CD** â€“ enforce testing, linting, and formatting via **GitHub Actions**.

This pipeline is designed for **scalability, reproducibility, and automation**, ensuring smooth collaboration between Data Science and DevOps teams.


### Dataset

* **Source**: LendingClub Loan Dataset (public).
* **Cleaned version**: `gs://loan-default-artifacts-loan-default-mlops/data/loan_default_selected_features_clean.csv`
* **Features**: loan amount, interest rate, credit grade, revolving balance, etc.
* **Target variable**: `loan_status` (defaulted vs non-defaulted).

---
---

## ğŸ—ï¸ Architecture & Tools

### High-Level Workflow

![Workflow](./workflow.svg)

### High-Level Workflow

```mermaid 
flowchart LR
    D[ğŸ“Š Data (GCS)] --> T[ğŸ§  Training DAG]
    T --> R[ğŸ“¦ MLflow Registry (Staging)]
    T --> A1[ğŸ“‚ Artifacts to GCS]

    R --> Pm[ğŸ”„ Promote DAG]
    Pm --> Prod[ğŸ“¦ Registry (Production)]

    Prod --> S[ğŸš€ Serving API]
    S --> BP[ğŸ“ˆ Batch Predictions]

    BP --> A2[ğŸ“‚ Predictions to GCS]
    BP --> M[ğŸ›¡ï¸ Drift Detection (Evidently)]

    M --> A3[ğŸ“‘ Reports to GCS]
    M -- |drift or degrade| --> T
```

### Detailed Architecture

```mermaid
flowchart LR
    subgraph Data["ğŸ“Š GCS Data"]
        A[loan_default_selected_features_clean.csv]
        B[batch_input.csv]
    end

    subgraph Training["ğŸ§  Training DAG"]
        A --> T1[train_with_mlflow.py]
        T1 --> MLflow[(MLflow Tracking & Registry)]
        T1 --> Artifacts[(GCS Artifacts)]
    end

    subgraph Registry["ğŸ“¦ MLflow Registry"]
        MLflow --> Staging[(Staging Alias)]
        MLflow --> Production[(Production Alias)]
        Promote[Promote Model DAG]:::dag
        Staging --> Promote --> Production
    end

    subgraph Serving["ğŸš€ Model Serving"]
        Production --> API["MLflow REST API - Docker"]
    end

    subgraph Batch["ğŸ“ˆ Batch Prediction DAG"]
        B --> P1[batch_predict.py]
        P1 --> Predictions[(Predictions in GCS)]
        Predictions --> Marker[latest_prediction.txt]
    end

    subgraph Monitoring["ğŸ›¡ï¸ Monitoring DAG"]
        Marker --> M1[monitor_predictions.py + Evidently]
        A --> M1
        M1 --> Reports[(Reports in GCS)]
    end

    classDef dag fill:#f9f,stroke:#333,stroke-width:2px;
```
ğŸ”‘ **Key Design Choice: Model Serving**

Instead of building a custom **FastAPI** or **Flask service**, this project leverages **MLflowâ€™s built-in REST API** for serving models.

  âœ… **Standardized interface** â†’ Follows MLflowâ€™s model format and signature.

  âœ… **Less boilerplate** â†’ No need to maintain custom inference logic.

  âœ… **CI/CD-friendly** â†’ Integration tests directly hit the `/invocations endpoint.

  âœ… **Portable** â†’ The same model can be served locally, in Docker, or in cloud environments without modification.

---
---

## âš™ï¸ Tech Stack

This project integrates **Machine Learning**, **MLOps**, and **Cloud Infrastructure** tools to deliver a production-ready loan default prediction system.

* **Programming Language**:

  * [Python 3.10+](https://www.python.org/) (core development)

* **Machine Learning & Experiment Tracking**:

  * [scikit-learn](https://scikit-learn.org/) â€“ model training & preprocessing
  * [XGBoost](https://xgboost.readthedocs.io/) â€“ gradient boosting models
  * [Optuna](https://optuna.org/) â€“ hyperparameter tuning
  * [MLflow](https://mlflow.org/) â€“ experiment tracking, model registry, and serving

* **Orchestration & Workflow Management**:

  * [Apache Airflow](https://airflow.apache.org/) â€“ DAGs for training, batch prediction, monitoring, and promotion
  * Custom **bash scripts** (`start_all.sh`, `stop_all.sh`) for container orchestration

* **Model Monitoring & Drift Detection**:

  * [Evidently AI](https://evidentlyai.com/) â€“ monitoring, data drift, and target drift reports

* **Infrastructure & Deployment**:

  * [Docker](https://www.docker.com/) â€“ containerization for all services
  * [Terraform](https://www.terraform.io/) â€“ Infrastructure as Code (IaC) on GCP
  * [Google Cloud Storage (GCS)](https://cloud.google.com/storage) â€“ storing artifacts, reports, and batch predictions
  * [GitHub Actions](https://docs.github.com/en/actions) â€“ CI/CD pipeline

* **Testing & Quality Assurance**:

  * [pytest](https://docs.pytest.org/) â€“ unit and integration tests
  * [flake8](https://flake8.pycqa.org/) â€“ linting
  * [black](https://black.readthedocs.io/) â€“ code formatting
  * [isort](https://pycqa.github.io/isort/) â€“ import sorting
  * [mypy](https://mypy.readthedocs.io/) â€“ static type checking

---
---

## ğŸ“‚ Repository Structure

```bash
loan_default_prediction/
â”œâ”€â”€ .github/workflows/        # CI/CD pipeline (GitHub Actions)
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ airflow/                  # Airflow orchestration environment
â”‚   â”œâ”€â”€ dags/                 # Training, batch prediction, monitoring DAGs
â”‚   â”‚   â”œâ”€â”€ train_pipeline_dag.py
â”‚   â”‚   â”œâ”€â”€ batch_prediction_dag.py
â”‚   â”‚   â”œâ”€â”€ monitoring_dag.py
â”‚   â”‚   â””â”€â”€ promote_model_dag.py
â”‚   â”œâ”€â”€ docker-compose.yaml   # Local Airflow + MLflow + GCS setup
â”‚   â”œâ”€â”€ start_all.sh          # Start Airflow + services
â”‚   â”œâ”€â”€ stop_all.sh           # Stop Airflow + services
â”‚   â”œâ”€â”€ start_serve.sh        # Start MLflow serving container
â”‚   â”œâ”€â”€ stop_serve.sh         # Stop MLflow serving container
â”‚   â”œâ”€â”€ logs/                 # Airflow runtime logs
â”‚   â”œâ”€â”€ artifacts/            # Airflow-local artifacts (predictions, reports)
â”‚   â””â”€â”€ mlruns/               # Local MLflow tracking (if not using GCS)
â”‚
â”œâ”€â”€ data/                     # Input data
â”‚   â”œâ”€â”€ batch_input.csv       # Example batch input
â”‚   â””â”€â”€ sample_input.json     # Sample request for prediction API
â”‚
â”œâ”€â”€ infra/terraform/          # IaC for GCP resources
â”‚   â”œâ”€â”€ main.tf               # Core infra definitions
â”‚   â”œâ”€â”€ variables.tf          # Parameterized variables
â”‚   â”œâ”€â”€ outputs.tf            # Outputs for GCS, services, etc.
â”‚   â””â”€â”€ terraform.tfvars      # Environment-specific variables
â”‚
â”œâ”€â”€ src/                      # Core ML code
â”‚   â”œâ”€â”€ train_with_mlflow.py  # Train & log models with MLflow
â”‚   â”œâ”€â”€ tune_xgboost_with_optuna.py  # Hyperparameter tuning
â”‚   â”œâ”€â”€ train.py              # Baseline model training
â”‚   â”œâ”€â”€ train_and_compare.py  # Compare multiple models
â”‚   â”œâ”€â”€ batch_predict.py      # Batch inference script
â”‚   â”œâ”€â”€ monitor_predictions.py # Drift detection & monitoring reports
â”‚   â”œâ”€â”€ predict.py            # Local test client for serving API
â”‚   â”œâ”€â”€ utils.py              # Shared helpers
â”‚   â””â”€â”€ config/               # (Optional) config files
â”‚
â”œâ”€â”€ tests/                    # Unit & integration tests
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â”œâ”€â”€ test_prediction_integration.py
â”‚   â”œâ”€â”€ test_batch_prediction_integration.py
â”‚   â””â”€â”€ conftest.py (if needed)
â”‚
â”œâ”€â”€ notebooks/                # EDA & experimentation notebooks
â”‚
â”œâ”€â”€ requirements.txt          # Core dependencies
â”œâ”€â”€ requirements-dev.txt      # Dev/test dependencies
â”œâ”€â”€ requirements-serve.txt    # MLflow serving dependencies
â”œâ”€â”€ requirements-monitoring.txt # Evidently/monitoring dependencies
â”œâ”€â”€ Dockerfile                # Base image for training/serving
â”œâ”€â”€ Dockerfile.airflow        # Airflow image
â”œâ”€â”€ Dockerfile.serve          # MLflow serving image
â”œâ”€â”€ Dockerfile.monitor        # Monitoring image
â”œâ”€â”€ Dockerfile.terraform      # Terraform-in-Docker image
â”œâ”€â”€ Makefile                  # Automation for linting, tests, Airflow, Terraform
â”œâ”€â”€ README.md                 # Project documentation (you are here ğŸš€)
â”œâ”€â”€ .flake8                   # Linting config
â”œâ”€â”€ pyproject.toml             # Black/isort/mypy config
â”œâ”€â”€ .gitignore                # Ignored files
â””â”€â”€ LICENSE                   # License file
```

### ğŸ”‘ Key Directories

* **`airflow/`** â†’ Orchestrates ML pipelines via DAGs
* **`src/`** â†’ Core ML training, prediction, monitoring scripts
* **`infra/terraform/`** â†’ Cloud infrastructure as code (GCS buckets, service accounts, etc.)
* **`tests/`** â†’ Unit + integration tests for reliability
* **`data/`** â†’ Example input and sample request payloads

---
---

## âš™ï¸ Setup & Installation

### Clone the Repository

```bash
git clone https://github.com/JDede1/loan_default_prediction.git
cd loan_default_prediction
```

### Prerequisites

Ensure the following are installed:

* **Docker** (>= 24.0) & **Docker Compose**
* **Python** 3.10+ (if running locally outside Docker)
* **Make** (for convenience commands)
* **GCP Service Account Key** (JSON) with access to GCS

  * Save it under:

    * `keys/gcs-service-account.json` (root)
    * `airflow/keys/gcs-service-account.json` (for Airflow)


### Environment Variables

Copy the `.env` template and update with your values:

```bash
cp .env.example .env
```

Key variables:

```bash
# Airflow
AIRFLOW_UID=50000

# GCS bucket for ML artifacts
GCS_BUCKET=loan-default-artifacts-mlops
STORAGE_BACKEND=gcs

# Path to service account key inside Airflow
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcs-service-account.json
```

### Install Dependencies (Local Dev)

If you want to run code **outside Docker**:

```bash
# Core + Dev dependencies
make install
```

This installs:

* `requirements.txt` (core ML packages)
* `requirements-dev.txt` (linting, testing, type checking)

### Start Airflow + MLflow + GCS (via Docker)

From the repo root:

```bash
make start
```

This spins up:

* **Airflow Webserver** ([http://localhost:8080](http://localhost:8080))
* **MLflow Tracking Server** ([http://localhost:5000](http://localhost:5000))
* **MLflow Model Serving** ([http://localhost:5001](http://localhost:5001))
* **Postgres + Redis** (Airflow backend)

To stop services:

```bash
make stop
```

### Run Linting & Tests

```bash
# Check linting & formatting
make lint

# Auto-format code
make format

# Run tests
make test
```

Integration tests can also be run inside Airflow containers:

```bash
make integration-tests
```

### Terraform (GCP Infrastructure)

All infra is defined in `infra/terraform/`.
To deploy (from repo root):

```bash
make terraform-init
make terraform-plan
make terraform-apply
```

This provisions:

* GCS bucket(s) for artifacts
* Service accounts & IAM bindings

To tear down:

```bash
make terraform-destroy
```
---
---
## ğŸš€ Usage

This section shows how to run **training**, **serving**, **batch inference**, and **monitoring** â€” either via **Airflow** (recommended) or **manually** for quick local checks.

### Start the stack

From repo root:

```bash
make start
```

* Airflow UI â†’ [http://localhost:8080](http://localhost:8080)
* MLflow UI â†’ [http://localhost:5000](http://localhost:5000)
* Serving API â†’ [http://localhost:5001/invocations](http://localhost:5001/invocations)

Stop everything:

```bash
make stop
```

### Run pipelines via Airflow (recommended)

Open Airflow UI â†’ **[http://localhost:8080](http://localhost:8080)**

DAGs to run (in order):

1. **train\_model\_with\_mlflow**

   * Trains a model on GCS/local data
   * Logs metrics/artifacts to MLflow
   * Registers/updates `loan_default_model@staging`

2. **promote\_model\_dag** (optional)

   * Promotes `staging` â†’ `production` if thresholds are met

3. **batch\_prediction\_dag**

   * Runs batch inference using the selected alias (staging/production)
   * Saves `predictions_*.csv` to GCS and updates the â€œlatestâ€ marker

4. **monitoring\_dag**

   * Compares training data vs latest predictions with **Evidently**
   * Saves JSON/HTML reports to artifacts (and GCS if enabled)

> Tip: You can trigger DAG runs manually in the UI or let the cron schedules handle it.


### Train locally (manual)

If you want a quick local training run (outside Airflow), you can invoke the training script:

```bash
python src/train_with_mlflow.py \
  --train_path data/loan_default_selected_features_clean.csv \
  --experiment_name loan_default_experiment \
  --register_name loan_default_model \
  --register_alias staging
```

Results:

* Run appears in **MLflow UI**
* Registered model/alias updated (if `--register_name` provided)

### Serving API (real-time inference)

The **serve** container hosts an MLflow model server bound to the **Model Registry alias**.

#### Quick curl test

```bash
curl -X POST http://localhost:5001/invocations \
  -H "Content-Type: application/json" \
  -d @data/sample_input.json
```

Expected:

```json
{"predictions": [0, 1, ...]}
```

#### Python test

```bash
python src/predict.py
```

> If you update the `staging` alias in MLflow, the **same container** will serve the new version automatically.


### Batch prediction (manual)

Generate predictions for a CSV of rows and save to a timestamped file (and GCS if configured):

```bash
python src/batch_predict.py \
  --model_name loan_default_model \
  --alias staging \
  --input_path data/batch_input.csv \
  --output_path artifacts/predictions.csv
```

Output:

* Local file like `artifacts/predictions_YYYY-MM-DD_HH-MM-SS.csv`
* Uploaded to `gs://<bucket>/predictions/...` if `STORAGE_BACKEND=gcs`

### Monitoring (manual)

Compare **training** vs **latest predictions** with Evidently:

```bash
python src/monitor_predictions.py \
  --train_data_path data/loan_default_selected_features_clean.csv \
  --prediction_path artifacts/predictions_YYYY-MM-DD_HH-MM-SS.csv
```

Outputs:

* `artifacts/monitoring_report_*.json`
* `artifacts/monitoring_report_*.html` (human-readable)

If `STORAGE_BACKEND=gcs` and `GCS_BUCKET` are set, reports are also uploaded to GCS.

### Tests

#### Unit tests (fast)

```bash
make test
```

#### Integration tests (inside Airflow container)

These hit real containers (MLflow server, model registry):

```bash
make integration-tests
```

* Ensures the **serving API** responds
* Runs **batch prediction** end-to-end

> CI (GitHub Actions) runs **lint + unit tests** on every push/PR to `main`.

### Common flows

**Full local run (with Docker):**

```bash
make start
# In Airflow UI: trigger train_model_with_mlflow
# optional: trigger promote_model_dag
# then: trigger batch_prediction_dag
# then: trigger monitoring_dag
```

**One-off quick check:**

```bash
python src/train_with_mlflow.py --register_name loan_default_model --register_alias staging
python src/batch_predict.py --model_name loan_default_model --alias staging --input_path data/batch_input.csv --output_path artifacts/preds.csv
python src/monitor_predictions.py --train_data_path data/loan_default_selected_features_clean.csv --prediction_path artifacts/preds_*.csv
```
---
---

## ğŸš¦ CI/CD

We use **GitHub Actions** for continuous integration to ensure code quality and reliability across the project.  
Every **push** or **pull request** to the `main` branch triggers the CI pipeline, which runs the following stages:

1. **Linting & Formatting**
   * Ensures code consistency and style with **flake8**, **black**, and **isort**.

2. **Testing**
   * Runs **pytest** for unit and functional tests.
   * Integration tests (e.g., API calls to the MLflow model server) are supported and can be toggled with `RUN_INTEGRATION_TESTS`.

3. **CI Results**
   * Status is reported back to GitHub with pass/fail checks.
   * Badges in the README display the current CI status.

4. **Future CD Extensions** (Planned)
   * Automated **Docker builds** for Airflow, serving, and monitoring services.
   * Deployment to **Google Cloud Platform (GCP)** using Terraform.
   * Automatic model promotion and batch job scheduling triggered by registry updates.

---

```mermaid
flowchart TD
    A[Push/Pull Request to main] --> B[Linting & Formatting]
    B --> C[Testing]
    C --> D[CI Results Reported]
    D --> E[Badges Updated]
    D --> F[Future: Docker Builds + GCP Deployment]
````
---
---

## Infrastructure (Terraform)

This project uses **Terraform** to provision and manage Google Cloud resources required for MLflow, Airflow, and batch prediction pipelines.

### ğŸŒ Infrastructure Components

* **Google Cloud Storage (GCS)**

  * Stores MLflow artifacts (models, metrics, feature importance, plots, etc.)
  * Stores batch prediction outputs and monitoring reports

* **Service Account + IAM Roles**

  * Provides secure authentication for Airflow and MLflow to access GCS

* **State Management**

  * Terraform keeps track of resource configurations via `terraform.tfstate`
  * Ensures reproducible and version-controlled infrastructure

### ğŸ“‚ Terraform Directory

```bash
infra/terraform/
â”œâ”€â”€ main.tf              # Main configuration for GCP services
â”œâ”€â”€ variables.tf         # Input variables (e.g., project_id, region, bucket_name)
â”œâ”€â”€ outputs.tf           # Exported outputs (e.g., bucket URL)
â”œâ”€â”€ terraform.tfvars     # Custom values for variables (gitignored)
â”œâ”€â”€ terraform.tfstate    # State file (gitignored)
â””â”€â”€ .terraform/          # Local Terraform cache (gitignored)
```

### âš™ï¸ Terraform Commands

All Terraform commands are wrapped in the `Makefile` for simplicity.

```bash
# Initialize Terraform
make terraform-init

# Show planned changes
make terraform-plan

# Apply changes (provision resources)
make terraform-apply

# Destroy resources
make terraform-destroy
```

### ğŸ” Authentication

Terraform authenticates with GCP using the **service account key**:

* Stored at: `keys/gcs-service-account.json`
* Mounted into Airflow and MLflow containers
* Exported as environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS=keys/gcs-service-account.json
```

### ğŸ—ï¸ Visual Overview

```mermaid
flowchart TD
    subgraph GCP["â˜ï¸ Google Cloud Platform"]
        SA["ğŸ”‘ Service Account (IAM)"]
        GCS["ğŸ—„ï¸ GCS Bucket (loan-default-artifacts)"]
    end

    Dev["ğŸ’» Developer (Terraform CLI)"] -->|terraform apply| GCP
    Airflow["ğŸŒ€ Airflow DAGs"] -->|read/write| GCS
    MLflow["ğŸ“¦ MLflow Tracking/Registry"] -->|store artifacts| GCS
```
---
---

## Development & Contribution

We welcome contributions ğŸš€ to improve the project.
Follow the guidelines below to ensure consistency, code quality, and smooth collaboration.


### ğŸ› ï¸ Development Setup

Clone the repository and install dependencies:

```bash
git clone https://github.com/JDede1/loan_default_prediction.git
cd loan_default_prediction

# Install core + dev dependencies
make install
```

### ğŸ“¦ Code Quality

The project enforces **linting, formatting, and type checks**:

```bash
# Format code
make format

# Lint code
make lint

# Run tests
make test
```

### Tools Used

* **Black** â†’ Python code formatting
* **isort** â†’ Import sorting
* **Flake8** â†’ Style guide enforcement
* **Mypy** â†’ Static type checking
* **Pytest** â†’ Unit & integration testing


### ğŸ”€ Git Workflow

1. **Create a feature branch**

   ```bash
   git checkout -b feature/my-new-feature
   ```

2. **Commit changes with clear messages**

   ```bash
   git commit -m "Add feature: X with explanation"
   ```

3. **Push branch & open PR**

   ```bash
   git push origin feature/my-new-feature
   ```

4. **PR Review & Merge** â†’ Code is reviewed before merging into `main`.

### ğŸ§ª Testing

* Unit tests â†’ `tests/test_utils.py`
* Integration tests â†’ `tests/test_prediction_integration.py` & `tests/test_batch_prediction_integration.py`

To run integration tests locally:

```bash
RUN_INTEGRATION_TESTS=1 pytest -m integration -v
```

### ğŸ“œ Contribution Guidelines

* Write clear commit messages (conventional commits encouraged).
* Add/update tests for new features.
* Ensure `make lint format test` passes before submitting PR.
* Document new features in the **README** or inline code comments.

---

---

## Future Improvements

A forward-looking roadmap to strengthen robustness, scale, and MLOps maturity.

### ğŸ”¬ Modeling & Data

* **Feature store** (Feast) for consistent online/offline features.
* **Hyperparameter search at scale** (Optuna on Ray/Vertex Vizier); early stopping & pruning.
* **Model ensembling** or **stacking** (XGBoost + LightGBM + calibrated LR).
* **Bias & fairness checks** (responsible AI reports; subgroup performance).
* **Online inference features** (recent credit events, rolling aggregates via streaming).
* **Automated data quality gates** (Great Expectations checks in DAG pre-steps).

### ğŸ› ï¸ Automation & Orchestration

* **Promotion policy as code**: formalize thresholds/guards in a single YAML consumed by both Airflow and CI.
* **Blue/green or canary serving**: route % traffic to the candidate model; rollback on SLO breach.
* **Backfills** and **reprocessing DAGs** with idempotent runs & data versioning (Delta/BigQuery time travel).

### ğŸ“ˆ Monitoring & Observability

* Expand **Evidently** coverage: target + data drift, stability, PSI, population shifts.
* **Latency & availability SLOs** for serving; alerts in Slack/Email with run links.
* Centralized logs/metrics/traces (OpenTelemetry â†’ Grafana/Prometheus/Loki).

### â˜ï¸ Infrastructure & Scale

* **Artifact store hardening**: signed URLs, retention tiers, inventory reports.
* **Autoscaling** for serving (KEDA/HPA) and **GPU-ready** build variants (if needed).
* **Distributed training** (Ray, Dask, or Spark) when dataset volume grows.

### ğŸ” Security & Compliance

* Secrets via **GCP Secret Manager**; remove JSON keys from mounts.
* **Least-privilege service accounts**; per-environment isolation (dev/stage/prod).
* Audit trails on **model registry** changes (who/what/when) surfaced in PRs.

### ğŸ’¸ Cost & Efficiency

* Storage lifecycle policies (archive â†’ delete).
* Spot/preemptible training nodes; cache datasets & artifacts.
* CI caching (pip/pytest) and conditional workflows to avoid redundant runs.

### ğŸ§­ Governance & Reproducibility

* **Model cards** and **data cards** published per release.
* Data & model **lineage** (OpenLineage + Marquez, or built-in with Airflow).
* **Semantic versioning** for models (e.g., `v{major.minor.patch}`) synced to registry aliases.

---

---

## ğŸ™ Acknowledgments

I would like to sincerely thank the following for their guidance, encouragement, and inspiration throughout the course of this project:

* **My mentors and peers**, whose feedback and discussions provided invaluable insights.
* **The broader data science and MLOps community**, for sharing knowledge and best practices that shaped my approach.
* **Family and friends**, for their unwavering support and patience during the many long hours dedicated to building and refining this project.

---